{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main tutorial notebook. It is intended to introduce the basic machinery and how it is used.\n",
    "\n",
    "First, let's import everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "# sys.path.append('.')  # or full path to CompSep\n",
    "np.math = math  # Redirect numpy.math to the built-in math module\n",
    "\n",
    "import denoising\n",
    "import importlib\n",
    "denoising = importlib.reload(denoising)\n",
    "import utils\n",
    "utils = importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data. Since this is simulation data of log column density (in units of $1/cm^3$). If the data is already preprocessed, you can just import it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the column density map from file\n",
    "logN_H = np.load('Archive/Turb_3.npy')[1]  \n",
    "# logN_H = utils.downsample_by_four(logN_H)\n",
    "nx, ny = logN_H.shape\n",
    "\n",
    "# Define constant dust temperature\n",
    "T_d = 10  # Typical Planck dust temperature in K\n",
    "\n",
    "# int_val = []\n",
    "# for nu in nu_list:\n",
    "#     int_val.append(np.mean(modified_blackbody(logN_H, T_d, nu*1e9)))\n",
    "# int_val = np.array(int_val)\n",
    "\n",
    "# Observation frequency (e.g., 353 GHz in Hz)\n",
    "\n",
    "# Compute the mock observed intensity map in μK_CMB\n",
    "nu = (217,353)\n",
    "# I_nu_map_μK_nu1 = utils.modified_blackbody(logN_H, T_d, nu[0]*1e9)\n",
    "# I_nu_map_μK_nu2 = utils.modified_blackbody(logN_H, T_d, nu[1]*1e9)\n",
    "alpha_1 = utils.MBB_factor(T_d, nu[0]*1e9)\n",
    "alpha_2 = utils.MBB_factor(T_d, nu[1]*1e9)\n",
    "I_nu_map_μK_nu1 = 10**logN_H*alpha_1\n",
    "I_nu_map_μK_nu2 = 10**logN_H*alpha_2\n",
    "\n",
    "I_nu_map_μK = (I_nu_map_μK_nu1, I_nu_map_μK_nu2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays that we work with need to have dimension `(1, H, W)`.\n",
    "\n",
    "Now we define tuples of dust which will be our ground truth $s$, white noise, $n$, and finally cmb $c$. For simplicity the CMB here is modelled as a simple Gaussian random field with a falling power law for the power spectrum.\n",
    "\n",
    "Keep in mind that in general, you can just import your own contamination array, respecting the broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure inputs have shape (1, H, W)\n",
    "dust_nu1 = I_nu_map_μK_nu1[None, :, :]\n",
    "dust_nu2 = I_nu_map_μK_nu2[None, :, :]\n",
    "\n",
    "dust = (dust_nu1, dust_nu2)\n",
    "\n",
    "# --- CMB Parameters ---\n",
    "n_realizations = 100\n",
    "SNR = 1\n",
    "amplitude = 2.\n",
    "spectral_index = -1.7\n",
    "\n",
    "nx, ny = dust_nu1.shape[-2], dust_nu1.shape[-1]\n",
    "\n",
    "# --- Compute noise variances ---\n",
    "variance_nu1 = (np.std(dust_nu1) / SNR) ** 2\n",
    "variance_nu2 = (np.std(dust_nu2) / SNR) ** 2\n",
    "variance = (variance_nu1, variance_nu2)\n",
    "\n",
    "# --- Create contamination_arr with shape (n_realizations, 2, 1, H, W) ---\n",
    "contamination_arr = np.zeros((n_realizations, 2, 1, nx, ny), dtype=np.float32)\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    # Shared CMB: shape (1, H, W)\n",
    "    cmb_map = utils.generate_cmb_map(n_x=nx, n_y=ny, amplitude=amplitude, spectral_index=spectral_index)\n",
    "    cmb_map = cmb_map.cpu().numpy()[None, :, :]\n",
    "\n",
    "    # Independent noise: shape (1, H, W)\n",
    "    noise_nu1 = np.random.normal(0, np.sqrt(variance_nu1), (1, nx, ny))\n",
    "    noise_nu2 = np.random.normal(0, np.sqrt(variance_nu2), (1, nx, ny))\n",
    "\n",
    "    # Total contamination: shape (1, H, W)\n",
    "    contamination_arr[i, 0] = noise_nu1 + cmb_map\n",
    "    contamination_arr[i, 1] = noise_nu2 + cmb_map\n",
    "\n",
    "contamination_arr_nu1 = contamination_arr[:, 0]  # shape: (Mn, 1, H, W)\n",
    "contamination_arr_nu2 = contamination_arr[:, 1]  # shape: (Mn, 1, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create our data, given by the equation $d^\\nu = s^\\nu + c + n^\\nu$ for each frequency band $\\nu$. Notice that we are assuming that we can sample configures $c \\sim P(c)$ and $n \\sim P(n)$. While for this case this might be possible, in general for more complicated contaminations this may be more difficult. This is a strong assumption.\n",
    "\n",
    "We assume that $c$ is independent of $\\nu$ because its spectral energy function is that of a perfect black body, and therefore $c^\\nu = B(\\nu, T) c_0$, where $c_0$ is some configuration and all the frequency dependce factors out into the Planck function. Therefore, we can define units (called Kelvin CMB) where this is independent of frequency.\n",
    "\n",
    "The tuple contains the data for two channels, that is, $(d^{\\nu_1}, d^{\\nu_2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_nu1 = np.random.normal(0, np.sqrt(variance_nu1), dust_nu1.shape)\n",
    "noise_nu2 = np.random.normal(0, np.sqrt(variance_nu2), dust_nu2.shape)\n",
    "\n",
    "noise = (noise_nu1, noise_nu2)\n",
    "\n",
    "cmb_map = utils.generate_cmb_map(n_x=nx, n_y=ny, amplitude=amplitude, spectral_index=spectral_index)\n",
    "cmb_map = cmb_map.cpu().numpy()[None, :, :]\n",
    "\n",
    "data_nu1 = dust_nu1 + noise_nu1 + cmb_map\n",
    "data_nu2 = dust_nu2 + noise_nu2 + cmb_map\n",
    "\n",
    "# define target\n",
    "data = (data_nu1, data_nu2)\n",
    "\n",
    "# target image is the data\n",
    "image_target = data\n",
    "\n",
    "# definte initial maps for optimisation\n",
    "# image_init = None\n",
    "image_init = ( 1e-20*data[0] / alpha_1,)\n",
    "# image_init = (np.random.normal(0, 1e-5, size=data[0].shape).astype(np.float32),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ is an image, then $\\phi(x)$ denotes a set of summary statistics. For example, if can be the pixel average, and the pixel wide standard deviation. In that case, it would be $\\phi(x) = ( \\mu, \\sigma )$. We are going to work with a different kind of coefficients, called Scattering Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholding = False\n",
    "if thresholding:\n",
    "    M, N, J, L = dust_nu1.shape[-2], dust_nu1.shape[-1], 7, 4\n",
    "    st_calc = denoising.Scattering2d(M, N, J, L)\n",
    "    st_calc.add_ref(ref=data_nu1)\n",
    "    s_cov = st_calc.scattering_cov(data_nu1, use_ref=True, \n",
    "                        normalization='P00', pseudo_coef=1\n",
    "                    )\n",
    "    st_calc.add_ref_ab(ref_a=data_nu1, ref_b=data_nu2)\n",
    "    s_cov_2fields = st_calc.scattering_cov_2fields(data_nu1, data_nu2, use_ref=True, \n",
    "                        normalization='P00'\n",
    "                    )\n",
    "    threshold_func = denoising.threshold_func(s_cov)\n",
    "    threshold_func_2fields = denoising.threshold_func(s_cov_2fields, two_fields=True)\n",
    "\n",
    "else:\n",
    "    threshold_func = None\n",
    "    threshold_func_2fields = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: If possible, try to parallelize compute_std in order to do the computation in parallel for each image instead of sequentially. \n",
    "#TODO: Make it compute the mean in addition to the std\n",
    "#TODO: Choose the indices in closure, and then pass the batch directly to the loss function, instead of the full contamination_array, and then choosing within each loss computation.\n",
    "std = denoising.compute_std(image_target, contamination_arr = contamination_arr, s_cov_func = threshold_func)\n",
    "std_double = denoising.compute_std_double(image_target, contamination_arr = contamination_arr, s_cov_func = threshold_func_2fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "# of estimators:  54827\n",
      "Current Loss: 1.78e+01\n",
      "Grad mean: 0.0003782631829380989\n",
      "Current Loss: 1.79e+01\n",
      "Grad mean: 0.00037990667624399066\n",
      "Current Loss: 1.78e+01\n",
      "Grad mean: 0.0003790052141994238\n",
      "Current Loss: 1.76e+01\n",
      "Grad mean: 0.0003741915279533714\n",
      "Current Loss: 1.28e+01\n",
      "Grad mean: 0.00028530810959637165\n",
      "Current Loss: 1.04e+01\n",
      "Grad mean: 0.0002425425045657903\n",
      "Current Loss: 7.90e+00\n",
      "Grad mean: 0.0001941134687513113\n",
      "Current Loss: 6.03e+00\n",
      "Grad mean: 0.00015641986101400107\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3 #number of epochs\n",
    "# decontaminate\n",
    "for i in range(n_epochs):\n",
    "    print(f'Starting epoch {i+1}')\n",
    "    running_map = denoising.denoise(image_target, contamination_arr = contamination_arr, std = std, std_double=std_double, seed=0, print_each_step=True, steps = 25, n_batch = 25, s_cov_func=threshold_func, s_cov_func_2fields=threshold_func_2fields, image_init = image_init)\n",
    "\n",
    "    alpha_nu1 = utils.MBB_factor(T_d, nu[0]*1e9)\n",
    "    alpha_nu2 = utils.MBB_factor(T_d, nu[1]*1e9)\n",
    "\n",
    "    running_map = (alpha_nu1*running_map[0], alpha_nu2*running_map[0])\n",
    "\n",
    "    std = denoising.compute_std(running_map, contamination_arr = contamination_arr, s_cov_func = threshold_func)\n",
    "    std_double = denoising.compute_std_double(running_map, contamination_arr = contamination_arr, s_cov_func = threshold_func_2fields)\n",
    "\n",
    "#TODO change function so that you give it a list of predefined loss functions\n",
    "image_syn_nu1 = running_map[0]\n",
    "image_syn_nu2 = running_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tuples to NumPy arrays\n",
    "dust = np.stack([dust_nu1[0], dust_nu2[0]])  # Shape: (2, ...)\n",
    "data = np.stack([data_nu1[0], data_nu2[0]])  # Shape: (2, ...)\n",
    "image_denoised = np.stack([image_syn_nu1[0], image_syn_nu2[0]])  # Ensure it's an array\n",
    "\n",
    "cmb = True\n",
    "# Create an array of objects to preserve different shapes\n",
    "results = np.array([dust, data, image_denoised])\n",
    "np.save(f\"nu={nu}_cmb={cmb}_alpha\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "healpy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
