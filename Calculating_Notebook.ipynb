{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main tutorial notebook. It is intended to introduce the basic machinery and how it is used.\n",
    "\n",
    "First, let's import everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "# sys.path.append('.')  # or full path to CompSep\n",
    "np.math = math  # Redirect numpy.math to the built-in math module\n",
    "\n",
    "import denoising\n",
    "import importlib\n",
    "denoising = importlib.reload(denoising)\n",
    "import utils\n",
    "utils = importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data. Since this is simulation data of log column density (in units of $1/cm^3$). If the data is already preprocessed, you can just import it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the column density map from file\n",
    "# logN_H = np.load('Archive/Turb_3.npy')[1]  \n",
    "logN_H = np.random.normal(0, 1, (384, 256))\n",
    "# logN_H = utils.downsample_by_four(logN_H)\n",
    "nx, ny = logN_H.shape\n",
    "\n",
    "# Define constant dust temperature\n",
    "T_d = 10  # Typical Planck dust temperature in K\n",
    "\n",
    "# int_val = []\n",
    "# for nu in nu_list:\n",
    "#     int_val.append(np.mean(modified_blackbody(logN_H, T_d, nu*1e9)))\n",
    "# int_val = np.array(int_val)\n",
    "\n",
    "# Observation frequency (e.g., 353 GHz in Hz)\n",
    "\n",
    "# Compute the mock observed intensity map in μK_CMB\n",
    "nu = (217,353)\n",
    "I_nu_map_μK_nu1 = utils.modified_blackbody(logN_H, T_d, nu[0]*1e9)\n",
    "I_nu_map_μK_nu2 = utils.modified_blackbody(logN_H, T_d, nu[1]*1e9)\n",
    "I_nu_map_μK = (I_nu_map_μK_nu1, I_nu_map_μK_nu2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays that we work with need to have dimension `(1, H, W)`.\n",
    "\n",
    "Now we define tuples of dust which will be our ground truth $s$, white noise, $n$, and finally cmb $c$. For simplicity the CMB here is modelled as a simple Gaussian random field with a falling power law for the power spectrum.\n",
    "\n",
    "Keep in mind that in general, you can just import your own contamination array, respecting the broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure inputs have shape (1, H, W)\n",
    "dust_nu1 = I_nu_map_μK_nu1[None, :, :]\n",
    "dust_nu2 = I_nu_map_μK_nu2[None, :, :]\n",
    "\n",
    "dust = (dust_nu1, dust_nu2)\n",
    "\n",
    "# --- CMB Parameters ---\n",
    "n_realizations = 100\n",
    "SNR = 1\n",
    "amplitude = 2.\n",
    "spectral_index = -1.7\n",
    "\n",
    "nx, ny = dust_nu1.shape[-2], dust_nu1.shape[-1]\n",
    "\n",
    "# --- Compute noise variances ---\n",
    "variance_nu1 = (np.std(dust_nu1) / SNR) ** 2\n",
    "variance_nu2 = (np.std(dust_nu2) / SNR) ** 2\n",
    "variance = (variance_nu1, variance_nu2)\n",
    "\n",
    "# --- Create contamination_arr with shape (n_realizations, 2, 1, H, W) ---\n",
    "contamination_arr = np.zeros((n_realizations, 2, 1, nx, ny), dtype=np.float32)\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    # Shared CMB: shape (1, H, W)\n",
    "    cmb_map = utils.generate_cmb_map(n_x=nx, n_y=ny, amplitude=amplitude, spectral_index=spectral_index)\n",
    "    cmb_map = cmb_map.cpu().numpy()[None, :, :]\n",
    "\n",
    "    # Independent noise: shape (1, H, W)\n",
    "    noise_nu1 = np.random.normal(0, np.sqrt(variance_nu1), (1, nx, ny))\n",
    "    noise_nu2 = np.random.normal(0, np.sqrt(variance_nu2), (1, nx, ny))\n",
    "\n",
    "    # Total contamination: shape (1, H, W)\n",
    "    contamination_arr[i, 0] = noise_nu1 + cmb_map\n",
    "    contamination_arr[i, 1] = noise_nu2 + cmb_map\n",
    "\n",
    "contamination_arr_nu1 = contamination_arr[:, 0]  # shape: (Mn, 1, H, W)\n",
    "contamination_arr_nu2 = contamination_arr[:, 1]  # shape: (Mn, 1, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create our data, given by the equation $d^\\nu = s^\\nu + c + n^\\nu$ for each frequency band $\\nu$. Notice that we are assuming that we can sample configures $c \\sim P(c)$ and $n \\sim P(n)$. While for this case this might be possible, in general for more complicated contaminations this may be more difficult. This is a strong assumption.\n",
    "\n",
    "We assume that $c$ is independent of $\\nu$ because its spectral energy function is that of a perfect black body, and therefore $c^\\nu = B(\\nu, T) c_0$, where $c_0$ is some configuration and all the frequency dependce factors out into the Planck function. Therefore, we can define units (called Kelvin CMB) where this is independent of frequency.\n",
    "\n",
    "The tuple contains the data for two channels, that is, $(d^{\\nu_1}, d^{\\nu_2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_nu1 = np.random.normal(0, np.sqrt(variance_nu1), dust_nu1.shape)\n",
    "noise_nu2 = np.random.normal(0, np.sqrt(variance_nu2), dust_nu2.shape)\n",
    "\n",
    "noise = (noise_nu1, noise_nu2)\n",
    "\n",
    "cmb_map = utils.generate_cmb_map(n_x=nx, n_y=ny, amplitude=amplitude, spectral_index=spectral_index)\n",
    "cmb_map = cmb_map.cpu().numpy()[None, :, :]\n",
    "\n",
    "data_nu1 = dust_nu1 + noise_nu1 + cmb_map\n",
    "data_nu2 = dust_nu2 + noise_nu2 + cmb_map\n",
    "\n",
    "# define target\n",
    "data = (data_nu1, data_nu2)\n",
    "\n",
    "# target is \n",
    "image_target = data\n",
    "\n",
    "# definte initial maps for optimisation\n",
    "image_init = image_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ is an image, then $\\phi(x)$ denotes a set of summary statistics. For example, if can be the pixel average, and the pixel wide standard deviation. In that case, it would be $\\phi(x) = ( \\mu, \\sigma )$. We are going to work with a different kind of coefficients, called  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholding = False\n",
    "if thresholding:\n",
    "    M, N, J, L = dust_nu1.shape[-2], dust_nu1.shape[-1], 7, 4\n",
    "    st_calc = denoising.Scattering2d(M, N, J, L)\n",
    "    st_calc.add_ref(ref=data_nu1)\n",
    "    s_cov = st_calc.scattering_cov(data_nu1, use_ref=True, \n",
    "                        normalization='P00', pseudo_coef=1\n",
    "                    )\n",
    "    st_calc.add_ref_ab(ref_a=data_nu1, ref_b=data_nu2)\n",
    "    s_cov_2fields = st_calc.scattering_cov_2fields(data_nu1, data_nu2, use_ref=True, \n",
    "                        normalization='P00'\n",
    "                    )\n",
    "    threshold_func = denoising.threshold_func(s_cov)\n",
    "    threshold_func_2fields = denoising.threshold_func(s_cov_2fields, two_fields=True)\n",
    "\n",
    "else:\n",
    "    threshold_func = None\n",
    "    threshold_func_2fields = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: If possible, try to parallelize compute_std in order to do the computation in parallel for each image instead of sequentially. \n",
    "#TODO: Make it compute the mean in addition to the std\n",
    "#TODO: Choose the indices in closure, and then pass the batch directly to the loss function, instead of the full contamination_array, and then choosing within each loss computation.\n",
    "std = denoising.compute_std(image_target, contamination_arr = contamination_arr, s_cov_func = threshold_func)\n",
    "std_double = denoising.compute_std_double(image_target, contamination_arr = contamination_arr, s_cov_func = threshold_func_2fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "# of estimators:  54827\n",
      "Current Loss: 1.25e+01\n",
      "Current Loss: 4.61e+03\n",
      "Current Loss: 2.95e+03\n",
      "Current Loss: 1.40e+03\n",
      "Current Loss: 8.37e+02\n",
      "Current Loss: 5.03e+02\n",
      "Current Loss: 2.68e+02\n",
      "Current Loss: 1.73e+02\n",
      "Current Loss: 1.14e+02\n",
      "Current Loss: 9.02e+01\n",
      "Current Loss: 3.33e+01\n",
      "Current Loss: 2.69e+01\n",
      "Current Loss: 2.63e+01\n",
      "Current Loss: 1.57e+01\n",
      "Current Loss: 1.65e+01\n",
      "Current Loss: 1.57e+01\n",
      "Current Loss: 1.59e+01\n",
      "Current Loss: 1.57e+01\n",
      "Current Loss: 1.57e+01\n",
      "Current Loss: 1.59e+01\n",
      "Current Loss: 1.47e+01\n",
      "Current Loss: 1.61e+01\n",
      "Current Loss: 1.64e+01\n",
      "Current Loss: 1.64e+01\n",
      "Current Loss: 1.52e+01\n",
      "Time used:  2792.017720222473 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     running_map \u001b[38;5;241m=\u001b[39m denoising\u001b[38;5;241m.\u001b[39mdenoise_double(image_target, contamination_arr \u001b[38;5;241m=\u001b[39m contamination_arr, std \u001b[38;5;241m=\u001b[39m std, std_double\u001b[38;5;241m=\u001b[39mstd_double, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, print_each_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, n_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, s_cov_func\u001b[38;5;241m=\u001b[39mthreshold_func, s_cov_func_2fields\u001b[38;5;241m=\u001b[39mthreshold_func_2fields)\n\u001b[1;32m      7\u001b[0m     std \u001b[38;5;241m=\u001b[39m denoising\u001b[38;5;241m.\u001b[39mcompute_std(running_map, contamination_arr \u001b[38;5;241m=\u001b[39m contamination_arr, s_cov_func \u001b[38;5;241m=\u001b[39m threshold_func)\n\u001b[0;32m----> 8\u001b[0m     std_double \u001b[38;5;241m=\u001b[39m \u001b[43mdenoising\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_std_double\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunning_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontamination_arr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontamination_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_cov_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold_func_2fields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#TODO change function so that you give it a list of predefined loss functions\u001b[39;00m\n\u001b[1;32m     11\u001b[0m image_syn_nu1 \u001b[38;5;241m=\u001b[39m running_map[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ScatteringDenoising/denoising/__init__.py:212\u001b[0m, in \u001b[0;36mcompute_std_double\u001b[0;34m(image, contamination_arr, image_ref, s_cov_func, J, L, M, N, l_oversampling, frequency_factor, device, wavelets, seed, if_large_batch, C11_criteria, normalization, precision, remove_edge)\u001b[0m\n\u001b[1;32m    209\u001b[0m     std_dev \u001b[38;5;241m=\u001b[39m COEFFS\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m std_dev\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstd_func_dual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_ref\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_ref\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ScatteringDenoising/denoising/__init__.py:205\u001b[0m, in \u001b[0;36mcompute_std_double.<locals>.std_func_dual\u001b[0;34m(x1, ref1, x2, ref2, Mn, batch_size)\u001b[0m\n\u001b[1;32m    202\u001b[0m     x2_noisy_batch \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m cont2[start_idx:end_idx]\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_idx \u001b[38;5;241m-\u001b[39m start_idx):\n\u001b[0;32m--> 205\u001b[0m         stats \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1_noisy_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_noisy_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    206\u001b[0m         COEFFS\u001b[38;5;241m.\u001b[39mappend(stats)\n\u001b[1;32m    208\u001b[0m COEFFS \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(COEFFS, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (Mn, N_coeffs)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ScatteringDenoising/denoising/__init__.py:168\u001b[0m, in \u001b[0;36mcompute_std_double.<locals>.func\u001b[0;34m(map1, ref_map1, map2, ref_map2)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Two-field case\u001b[39;00m\n\u001b[1;32m    166\u001b[0m st_calc\u001b[38;5;241m.\u001b[39madd_ref_ab(ref_a\u001b[38;5;241m=\u001b[39mref_map1, ref_b\u001b[38;5;241m=\u001b[39mref_map2)\n\u001b[0;32m--> 168\u001b[0m coef_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfunc_s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(coef_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ScatteringDenoising/denoising/__init__.py:146\u001b[0m, in \u001b[0;36mcompute_std_double.<locals>.func_s\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc_s\u001b[39m(x1, x2):\n\u001b[0;32m--> 146\u001b[0m         coeff_dict \u001b[38;5;241m=\u001b[39m \u001b[43mst_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscattering_cov_2fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_large_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_large_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC11_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC11_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_edge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_edge\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# select = ~torch.isin(result['index_for_synthesis'][0], torch.tensor([1, 3, 7, 11, 15, 19]))\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m coeff_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor_synthesis\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ScatteringDenoising/denoising/Scattering2d.py:991\u001b[0m, in \u001b[0;36mScattering2d.scattering_cov_2fields\u001b[0;34m(self, data_a, data_b, if_large_batch, C11_criteria, use_ref, normalization, remove_edge)\u001b[0m\n\u001b[1;32m    985\u001b[0m         norm_factor_C01_ab \u001b[38;5;241m=\u001b[39m (P00_a[:,\u001b[38;5;28;01mNone\u001b[39;00m,j3,:] \u001b[38;5;241m*\u001b[39m P11_b_temp)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    986\u001b[0m         norm_factor_C01_ba \u001b[38;5;241m=\u001b[39m (P00_b[:,\u001b[38;5;28;01mNone\u001b[39;00m,j3,:] \u001b[38;5;241m*\u001b[39m P11_a_temp)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    987\u001b[0m C01[:,\u001b[38;5;241m0\u001b[39m,j2,j3,:,:] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_a_f_small\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_image\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mM3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI1_a_f_small\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj2\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mM3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwavelet_f3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 991\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m fft_factor \u001b[38;5;241m/\u001b[39m norm_factor_C01_a\n\u001b[1;32m    992\u001b[0m C01[:,\u001b[38;5;241m1\u001b[39m,j2,j3,:,:] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    993\u001b[0m     data_b_f_small\u001b[38;5;241m.\u001b[39mview(N_image,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,M3,N3) \u001b[38;5;241m*\u001b[39m \n\u001b[1;32m    994\u001b[0m     torch\u001b[38;5;241m.\u001b[39mconj(I1_b_f_small[:,j2]\u001b[38;5;241m.\u001b[39mview(N_image,L,\u001b[38;5;241m1\u001b[39m,M3,N3)) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    995\u001b[0m     wavelet_f3\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,L,M3,N3)\n\u001b[1;32m    996\u001b[0m )\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m fft_factor \u001b[38;5;241m/\u001b[39m norm_factor_C01_b\n\u001b[1;32m    997\u001b[0m C01[:,\u001b[38;5;241m2\u001b[39m,j2,j3,:,:] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    998\u001b[0m     data_a_f_small\u001b[38;5;241m.\u001b[39mview(N_image,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,M3,N3) \u001b[38;5;241m*\u001b[39m \n\u001b[1;32m    999\u001b[0m     torch\u001b[38;5;241m.\u001b[39mconj(I1_b_f_small[:,j2]\u001b[38;5;241m.\u001b[39mview(N_image,L,\u001b[38;5;241m1\u001b[39m,M3,N3)) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m   1000\u001b[0m     wavelet_f3\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,L,M3,N3)\n\u001b[1;32m   1001\u001b[0m )\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m fft_factor \u001b[38;5;241m/\u001b[39m norm_factor_C01_ab\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 3 #number of epochs\n",
    "# decontaminate\n",
    "for i in range(n_epochs):\n",
    "    print(f'Starting epoch {i+1}')\n",
    "    running_map = denoising.denoise_double(image_target, contamination_arr = contamination_arr, std = std, std_double=std_double, seed=0, print_each_step=True, steps = 25, n_batch = 25, s_cov_func=threshold_func, s_cov_func_2fields=threshold_func_2fields)\n",
    "\n",
    "    std = denoising.compute_std(running_map, contamination_arr = contamination_arr, s_cov_func = threshold_func)\n",
    "    std_double = denoising.compute_std_double(running_map, contamination_arr = contamination_arr, s_cov_func = threshold_func_2fields)\n",
    "\n",
    "#TODO change function so that you give it a list of predefined loss functions\n",
    "image_syn_nu1 = running_map[0]\n",
    "image_syn_nu2 = running_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tuples to NumPy arrays\n",
    "dust = np.stack([dust_nu1[0], dust_nu2[0]])  # Shape: (2, ...)\n",
    "data = np.stack([data_nu1[0], data_nu2[0]])  # Shape: (2, ...)\n",
    "image_denoised = np.stack([image_syn_nu1[0], image_syn_nu2[0]])  # Ensure it's an array\n",
    "\n",
    "cmb = True\n",
    "# Create an array of objects to preserve different shapes\n",
    "results = np.array([dust, data, image_denoised])\n",
    "np.save(f\"nu={nu}_cmb={cmb}_threshold\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "healpy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
